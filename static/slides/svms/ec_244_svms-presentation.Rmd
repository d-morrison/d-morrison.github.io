---
title: "Support Vector Machines"
subtitle: "An Introduction"
author: "Emilie Campos, UCLA"
date: "2019-04-30"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: ""
    css: ["default", "metropolis", "metropolis-fonts"]
    includes: 
      in_header: header.html
editor_options: 
  chunk_output_type: inline
header_includes: 
  -\usepackage{amsmath}
---

<style type="text/css">
.remark-slide-content {
    font-size: 25px;
    padding: 1em 4em 1em 4em;
}

.pull-left {
  float: left;
  width: 47%;
}
.pull-right {
  float: right;
  width: 47%;
}
</style>

```{r setup, include=FALSE}
library(ggplot2)
library(tidyverse)
library(e1071)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  echo = FALSE,
  fig.asp = .6, 
  fig.width = 12
)
theme_set(theme_bw() + theme(text = element_text(size = 20)))
```

# Overview 

Introduction to Statistical Learning with Applications in R (ISLR), Chapter 9:

- Maximal Margin Classifier 

- Support Vector Classifier 

- Support Vector Machines 


???

start from the most simple and generalize 

---
class: inverse, center, middle

# Maximal Margin Classifier 

???

the maximal margin classifer is the hyperplane that has the largest boundary between all of the points, the key point here is that the points are separable

---

# Maximal Margin Classifier 1

.center[
```{r maximal margin scatterplot}
set.seed(1234)
max_margin_data <- data.frame(
  x1 = c(rnorm(5, 0, 1), rnorm(5, 5, 1)),
  x2 = c(rnorm(5, 0, 1), rnorm(5, 5, 1)),
  y = factor(c(rep(-1, 5), rep(1, 5)),
    labels = c("Group 1", "Group 2")
  )
)
ggplot(max_margin_data) +
  geom_point(aes(x1, x2, color = y),
    size = 15
  ) +
  theme(legend.title = element_blank()) + 
  ggtitle("Linearly separable data") +
  theme(legend.position = "none")
```
]

???

it's easy to see here that this data is separable by a line

---

# Maximal Margin Classifier 2

.center[
```{r maximal margin hyperplanes}
ggplot(max_margin_data) +
  geom_point(aes(x1, x2, color = y),
    size = 15
  ) +
  theme(legend.title = element_blank()) +
  geom_abline(slope = -2, intercept = 6) +
  geom_abline(slope = -1, intercept = 4) +
  geom_abline(slope = 0.2, intercept = 2) +
  ggtitle("Linearly separable data") +
  theme(legend.position = "none")
```
]

$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$$

???

there are many separating lines, we want the one that is least likely to misclassify data which would be the one with the maximal margin between the data and the line 

However, we rarely work with one predictor. Luckily, a line is a hyperplane so we can easily extend our example to $p$ dimensions. 

---

# Maximal Margin Classifier 3

A *hyperplane* is a flat affine subspace of dimension $p-1$  

- in 2D: a line
$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 = 0$$
- in 3D: a plane
$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 = 0$$

- Generalized to the $p$-dimensional setting:
$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p = 0$$
???

affine subspace: similar to a linear subspace except there is no fixed origin, there are instead displacement vectors which are translations or subtractions of two points in space

after that, it's hard to visualize a hyperplane but the notion of a p-1 dimensional flat subspace still applies 

but what about points that don't lie on the hyperplane 

---

# Maximal Margin Classifier 4

A point can lie on either side of the hyperplane which would mean either  
$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p > 0$$
or 
$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p < 0$$
???

this is how we will build the maximal margin classifier 

---

# Maximal Margin Classifier 5

.center[
```{r maximal margin hyperplanes again}
grid <- expand.grid(
  seq(min(max_margin_data[, 1]),
    max(max_margin_data[, 1]),
    length.out = 1000
  ),
  seq(min(max_margin_data[, 2]),
    max(max_margin_data[, 2]),
    length.out = 1000
  )
)
names(grid) <- names(max_margin_data)[1:2]
df <- data.frame(grid) %>% 
  mutate(Prediction = case_when(6 - 2*x1 - x2 > 0 ~ "Group 1",
                                6 - 2*x1 - x2 < 0 ~ "Group 2"))
ggplot(df, aes(x = x1, y = x2)) +
  geom_tile(aes(fill = Prediction), alpha = 0.5) + 
  geom_point(data = max_margin_data,
             aes(x1, x2, color = y),
             size = 15) +
  theme(legend.title = element_blank()) +
  geom_abline(slope = -2, intercept = 6) +
  ggtitle("Linearly separable data") +
  theme(legend.position = "none")
```

$\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip} > 0 \text{ if } y_i = 1$  
$\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip} < 0 \text{ if } y_i = -1$
]

???

label the points in the teal blue group $y_i = 1$ and the points in the red group $y_i = -1$ 

---

# Maximal Margin Classifier 6

.center[
```{r maximal margin hyperplanes again 2}
ggplot(df, aes(x = x1, y = x2)) +
  geom_tile(aes(fill = Prediction), alpha = 0.5) + 
  geom_point(data = max_margin_data,
             aes(x1, x2, color = y),
             size = 15) +
  theme(legend.title = element_blank()) +
  geom_abline(slope = -2, intercept = 6) +
  ggtitle("Linearly separable data") +
  theme(legend.position = "none")
```

$y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip}) > 0$  
]

---

# Maximal Margin Classifier 7 

Goal: choose the hyperplane as the solution to this optimization problem

$$\begin{align}
& \text{maximize}_{\beta_0, \beta_1, ..., \beta_p} M \\ 
& \text{subject to } \sum_{j=1}^p \beta_j^2 = 1, \\
& y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... + \beta_px_{ip}) \geq M \; \forall \; i\\ 
\end{align}$$

---

# Maximal Margin Classifier 8

.center[
```{r maximal margin classifier svm}
svmfit <- svm(y ~ x1 + x2,
  data = max_margin_data,
  kernel = "linear",
  cost = 100
)
grid <- expand.grid(
  seq(min(max_margin_data[, 1]),
    max(max_margin_data[, 1]),
    length.out = 1000
  ),
  seq(min(max_margin_data[, 2]),
    max(max_margin_data[, 2]),
    length.out = 1000
  )
)
names(grid) <- names(max_margin_data)[1:2]
Prediction <- predict(svmfit, grid)
df <- data.frame(grid, Prediction)

max_margin_data$support <- "not support"
max_margin_data[svmfit$index, "support"] <- "support"

cols <- c("Group 1" = "red", "Group 2" = "black")
tiles <- c("Group 1" = "magenta", "Group 2" = "cyan")
shapes <- c("support" = 4, "not support" = 1)

ggplot(df, aes(x = x1, y = x2)) +
  geom_tile(aes(fill = Prediction), alpha = 0.5) +
  geom_point(
    data = max_margin_data,
    aes(color = y, shape = support),
    size = 15
  ) +
  scale_color_manual(values = cols) +
  scale_shape_manual(values = shapes) +
  ggtitle("SVM Classification plot") +
  theme(legend.position = "none")
```
]

???

unfortunately, the svm function in R does not give the coefficients of the linear boundary 

the x's are the support vectors 

what happens though, if our data is not linearly separable? we need to allow some points to violate this boundary.
---
class: inverse, center, middle

# Support Vector Classifier 

???

there are two cases where a separating hyperplane is not desirable -- the obvious: when the data is not linearly separable, and the not as obvious: when an additional data point will change the hyperplane drastically and results in overfitting the data 

the support vector classifier uses a *soft margin* that allows some points to be misclassified AT A COST

---

# Support Vector Classifier 1

Goal: choose the hyperplane as the solution to this optimization problem

$$\begin{align}
& \text{maximize}_{\beta_0, \beta_1, ..., \beta_p, \epsilon_1, ..., \epsilon_n} M \\ 
& \text{subject to } \sum_{j=1}^p \beta_j^2 = 1, \\
& y_i(\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... + \beta_px_{ip}) \geq M(1-\epsilon_i) \\ 
& \epsilon_i \geq 0, \sum_{i=1}^n \epsilon_i \leq C, 
\end{align}$$

where $C$ is a nonnegative tuning parameter and $M$ is the width of the margin 

???

the epsilons are *slack variables*, they allow individual observations to be on the wrong side of the hyperplane but note that the sum of all of these slack variables has to be less than our cost $C$ 

correct side of the margin, slack is 0
wrong side of the margin, epsilon is greater than 0
wrong side of the hyperplane, slack is greater than 1 and is misclassified

again, once we have the separating hyperplane, we classify observations in the same way as before, based on the sign 

in practice, $C$ is chosen using cross-validation and it controls the bias-variance trade off 

when $C$ is small, the margins are narrow and there are rarely any violations but the classifier is highly fit to the data (low bias, high variance)

when $C$ is large, the margins are wide and there are more violations and misclassifications (high bias, lower variance)

---

# Support Vector Classifier 2 

.center[
```{r}
set.seed(1234)
support_class_data <- data.frame(
  x1 = c(rnorm(5, 0, 1), rnorm(5, 1, 1)),
  x2 = c(rnorm(5, 0, 1), rnorm(5, 1, 1)),
  y = factor(c(rep(-1, 5), rep(1, 5)),
    labels = c("Group 1", "Group 2")
  )
)
ggplot(support_class_data) +
  geom_point(aes(x1, x2, color = y),
             size = 15) +
  theme(legend.title = element_blank()) + 
  ggtitle("Non-separable data") +
  theme(legend.position = "none")
```
]

---

# Support Vector Classifier 3 

.center[
```{r support vector classifier svm}
svmfit <- svm(y ~ x1 + x2,
  data = support_class_data,
  kernel = "linear",
  cost = 100
)

grid <- expand.grid(
  seq(min(support_class_data[, 1]),
    max(support_class_data[, 1]),
    length.out = 1000
  ),
  seq(min(support_class_data[, 2]),
    max(support_class_data[, 2]),
    length.out = 1000
  )
)
names(grid) <- names(support_class_data)[1:2]
Prediction <- predict(svmfit, grid)
df <- data.frame(grid, Prediction)

support_class_data$support <- "not support"
support_class_data[svmfit$index, "support"] <- "support"

cols <- c("Group 1" = "red", "Group 2" = "black")
shapes <- c("support" = 4, "not support" = 1)

ggplot(df, aes(x = x1, y = x2)) +
  geom_tile(aes(fill = Prediction), alpha = 0.5) +
  geom_point(data = support_class_data,
             aes(color = y, shape = support),
             size = 15) +
  scale_color_manual(values = cols) +
  scale_shape_manual(values = shapes) +
  ggtitle("SVM Classification plot") +
  theme(legend.position = "none")
```
]

---
class: inverse, center, middle

# Support Vector Machines

???

we've been dealing entirely with linear boundaries, but we can extend this classifier to a non-linear boundary 

---

# Support Vector Machines 1

Rather than fitting a support vector classifier using $p$ features $$X_1, ..., X_p$$ we could instead fit a support vector classifier using $2p$ features $$X_1, X_1^2, X_2, X_2^2, ..., X_p, X_p^2$$ which is linear in the new feature space but non-linear in the original feature space. However, polynomials are not the only option.

???

instead, we're going to want to use *kernels*

---

# Support Vector Classifier Revisted

The inner product of two vectors $x_i$ and $x_{i^\prime}$ is $\langle x_i, x_{i^\prime}\rangle = \sum_{j=1}^p x_{ij}x_{i^\prime j}.$ The support vector classifier can be rewritten using *inner products* $$f(x) = \beta_0 + \sum_{i=1}^n \alpha_i \langle x, x_i\rangle$$ where there are $n$ parameters $\alpha_i$.

To estimate the $\alpha_i$ and $\beta_0$, we need all ${n \choose 2}$ inner products $\langle x_i, x_{i^\prime}\rangle$. Turns out, $\alpha_i$ is nonzero only for the support vectors in the solution so if $\mathbb{S}$ is the set of support vectors then $$f(x) = \beta_0 + \sum_{i\in \mathbb{S}}\alpha_i\langle x, x_i\rangle.$$ 


---

# Support Vector Machines 2

Remember how the support vector classifier was rewritten $$f(x) = \beta_0 + \sum_{i\in \mathbb{S}}\alpha_i\langle x, x_i\rangle.$$ Replace the inner product with a generalization of the form $K(x_i, x_{i^\prime})$ where $K$ is some function referred to as the *kernel*.  

---

# Support Vector Machines 3 

The support vector classifier is known as a *linear* kernel. Other possible kernels include:

- *polynomial kernel*: $$K(x_i, x_{i^\prime}) = (1+\sum_{j=1}^p x_{ij}x_{i^\prime j})^d$$

- *radial kernel*: $$K(x_i, x_{i^\prime}) = \exp(-\gamma\sum_{j=1}^p(x_{ij} - x_{i^\prime j})^2)$$

???

linear kernel: essentially quantifies the similarity of a pair of observations using Pearson correlation 

polynomial: more flexible, d=1 is the linear kernel and d>1 will be higher degree polynomials 

radial: gamma is a positive constant, radial kernel has very local behavior in the sense that only nearby training observations have an effect on the class label of a test observation 

why are kernels good: we don't need to compute all of the parameters in the enlarged feature space, which can become computationally infeasible 

---

# Support Vector Machines 4 

```{r}
set.seed(1) 
svm_data <- data.frame(x1 = rnorm(200), 
                       x2 = rnorm(200))
svm_data[1:100, ] <- svm_data[1:100, ] + 2
svm_data[101:150, ] <- svm_data[101:150, ] - 2
svm_data <- mutate(svm_data, 
                   y = factor(c(rep(1, 150), rep(2, 50)), 
                              labels = c("Group 1", "Group 2")))

ggplot(svm_data) + 
  geom_point(aes(x = x1, y = x2, color = y), 
             size = 15) + 
  theme(legend.position = "none") + 
  ggtitle("Non-linear boundary data")
```

---

# Support Vector Machines 5 

```{r}
train <- sample(200, 100) 
svm_data_train <- svm_data[train, ]
svm_data_test <- svm_data[-train, ]

tune.out <- tune(svm, y ~ ., data = svm_data_train, 
                 kernel = "radial",
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))
svm_best <- tune.out$best.model

grid <- expand.grid(
  seq(min(svm_data[, 1]),
    max(svm_data[, 1]),
    length.out = 1000
  ),
  seq(min(svm_data[, 2]),
    max(svm_data[, 2]),
    length.out = 1000
  )
)
names(grid) <- names(svm_data)[1:2]
Prediction <- predict(svm_best, grid)
df <- data.frame(grid, Prediction)

svm_data_train$support <- "not support"
svm_data_train[svm_best$index, "support"] <- "support"

cols <- c("Group 1" = "red", "Group 2" = "black")
shapes <- c("support" = 4, "not support" = 1)

ggplot(df, aes(x = x1, y = x2)) +
  geom_tile(aes(fill = Prediction), alpha = 0.5) +
  geom_point(data = svm_data_train,
             aes(color = y, shape = support),
             size = 15) +
  scale_color_manual(values = cols) +
  scale_shape_manual(values = shapes) +
  ggtitle("SVM Classification plot") +
  theme(legend.position = "none")
```

---

# Support Vector Machine 6

```{r}
svm_data_test <- svm_data_test %>% 
  mutate(Prediction = predict(svm_best, newdata = svm_data_test), 
         Misclassified = factor(ifelse(y == Prediction, 0, 1)))

ggplot(df, aes(x = x1, y = x2)) +
  geom_tile(aes(fill = Prediction), alpha = 0.5) +
  geom_point(data = svm_data_test,
             aes(color = y, shape = Misclassified),
             size = 15) +
  scale_color_manual(values = cols) +
  scale_shape_manual(values = c('1' = 4, '0' = 1)) +
  ggtitle("SVM Classification plot - Test set") +
  theme(legend.position = "none")

classification_table <- table(true = svm_data[-train, "y"], 
      pred = predict(svm_best, newdata = svm_data[-train ,]))
```

Here, the crosses represent misclassifications. `r (classification_table[1, 2] + classification_table[2, 1])/100 * 100`\\% of the test observations were misclassified by this SVM. 

---

# Connection to Logistic Regression

The SVM can be rewritten as 
$$\text{minimize}_{\beta_0, \beta_1, ..., \beta_p}\large{\sum_{i=1}^n[0,1-y_if(x_i)] + \lambda\sum_{j=1}^p\beta_j^2\large}$$
where $\lambda$ is a nonnegative tuning parameter

???

when SVMs were first introduced, they were incredibly popular due to their good performance as well as the fact that they were novel and mysterious 

however, there are deep connections between SVMs and more classical methods such as 

when lambda is large then the betas are small, more violations to the margin are tolerated and a low-variance but high-bias classifier will result 

when lambda is small then few violations will occur 

so lambda plays the role similar to the cost $C$ 

the first term is the loss function, known as hinge loss, closely related to the loss function used in logistic regression

when classes are well separated, SVMs tend to perform better than logisitic regression and when they are more overlapping, it is better to use logisitic regression

use the right tool for the job, it's common to talk about machine learning to sound hip and advanced 

---

# SVMs with More than Two Classes

Two proposed approaches: 

- One-Versus-One Classification: $K>2$ classes, construct ${K \choose 2}$ SVMs, tally how many times class $k$ is chosen for observation $k$

- One-Versus-All Classification: $K>2$ classes, construct $K$ SVMs in which class $k$ is compared to the remaining $k-1$ classes, assign observation $x^\star$ to the class for which $\beta_{0k} + \beta_{1k}x_1^\star + ... + \beta_{pk}x^\star_p$ is the largest 

???

so far I've only described problems with a binary classification setting, and the SVM idea of separating hyperplanes does not generalize easily to more than two classes 

---

class: inverse, center, middle

# Keep in touch

<a href="mailto:ejcampos@ucla.edu"><i class="fa fa-paper-plane fa-fw"></i>&nbsp; ejcampos@ucla.edu</a><br>
<a href="https://emjcampos.netlify.com"><i class="fa fa-link fa-fw"></i>&nbsp; emjcampos.netlify.com</a><br>
<a href="http://twitter.com/emjcampos"><i class="fa fa-twitter fa-fw"></i>&nbsp; @emjcampos</a><br>
<a href="http://github.com/emjcampos"><i class="fa fa-github fa-fw"></i>&nbsp; @emjcampos</a><br>

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).
